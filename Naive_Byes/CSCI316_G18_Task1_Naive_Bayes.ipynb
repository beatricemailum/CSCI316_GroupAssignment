{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/BigData_grp_assignment/apartments10.csv'\n",
        "\n",
        "\n",
        "df = pd.read_csv(path, sep=None, engine=\"python\", encoding=\"cp1252\")\n",
        "\n",
        "\n",
        "df.columns = (df.columns\n",
        "              .str.strip()\n",
        "              .str.lower()\n",
        "              .str.replace(r\"\\s+\", \"_\", regex=True))\n",
        "\n",
        "print(df.shape)\n",
        "df.head()\n",
        "\n",
        "print(\"\\n2. DATA PREPARATION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Remove price_display\n",
        "print(\"Removing 'price_display' \")\n",
        "if 'price_display' in df.columns:\n",
        "    df = df.drop('price_display', axis=1)\n",
        "\n",
        "# feature engineering, adding new columns\n",
        "class ApartmentFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, use_new_features=True, fitted_values=None):\n",
        "        self.use_new_features = use_new_features\n",
        "        self.fitted_values = fitted_values or {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.use_new_features:\n",
        "            # Store median values from training data only\n",
        "            if 'square_feet' in X.columns:\n",
        "                self.fitted_values['square_feet_median'] = X['square_feet'].median()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = X.copy()\n",
        "\n",
        "        if self.use_new_features:\n",
        "            # Feature 1: Square feet availability indicator\n",
        "            if 'square_feet' in X_transformed.columns:\n",
        "                X_transformed['has_square_feet'] = (~X_transformed['square_feet'].isna()).astype(int)\n",
        "                # Feature: Price per square foot ratio (log-transformed for better distribution)\n",
        "                sqft_fill_value = self.fitted_values.get('square_feet_median', X_transformed['square_feet'].median())\n",
        "                X_transformed['sqft_log'] = np.log1p(X_transformed['square_feet'].fillna(sqft_fill_value))\n",
        "\n",
        "            # Feature 2: Amenity count and specific amenities\n",
        "            if 'amenities' in X_transformed.columns:\n",
        "                amenities_filled = X_transformed['amenities'].fillna('')\n",
        "                X_transformed['amenity_count'] = amenities_filled.apply(\n",
        "                    lambda x: len([a.strip() for a in x.split(',') if a.strip()]) if x else 0\n",
        "                )\n",
        "                # indicators for valuable amenities\n",
        "                X_transformed['has_dishwasher'] = amenities_filled.str.contains('Dishwasher', case=False, na=False).astype(int)\n",
        "                X_transformed['has_elevator'] = amenities_filled.str.contains('Elevator', case=False, na=False).astype(int)\n",
        "                X_transformed['has_pool'] = amenities_filled.str.contains('Pool', case=False, na=False).astype(int)\n",
        "                X_transformed['has_parking'] = amenities_filled.str.contains('Parking|Garage', case=False, na=False).astype(int)\n",
        "\n",
        "            # Feature 3: Location-based features\n",
        "            if 'state' in X_transformed.columns:\n",
        "                high_cost_states = ['NY', 'CA', 'WA', 'MA']\n",
        "                medium_cost_states = ['DC', 'VA', 'IL']\n",
        "                X_transformed['high_cost_location'] = X_transformed['state'].isin(high_cost_states).astype(int)\n",
        "                X_transformed['medium_cost_location'] = X_transformed['state'].isin(medium_cost_states).astype(int)\n",
        "\n",
        "            # Feature 4: Room-based features\n",
        "            if 'bedrooms' in X_transformed.columns and 'bathrooms' in X_transformed.columns:\n",
        "                bedrooms_filled = X_transformed['bedrooms'].fillna(0)\n",
        "                bathrooms_filled = X_transformed['bathrooms'].fillna(0)\n",
        "                X_transformed['total_rooms'] = bedrooms_filled + bathrooms_filled\n",
        "                # Room efficiency ratio\n",
        "                X_transformed['room_efficiency'] = np.where(\n",
        "                    X_transformed['square_feet'].notna() & (X_transformed['square_feet'] > 0),\n",
        "                    X_transformed['total_rooms'] / (X_transformed['square_feet'] / 100),\n",
        "                    X_transformed['total_rooms'].fillna(0)\n",
        "                )\n",
        "                # Studio apartment indicator (0 bedrooms)\n",
        "                X_transformed['is_studio'] = (bedrooms_filled == 0).astype(int)\n",
        "\n",
        "            # Feature 5: Geographic clustering\n",
        "            if 'latitude' in X_transformed.columns and 'longitude' in X_transformed.columns:\n",
        "                # Create location clusters based on lat/lon\n",
        "                lat_fill_value = self.fitted_values.get('latitude_median', X_transformed['latitude'].median())\n",
        "                lon_fill_value = self.fitted_values.get('longitude_median', X_transformed['longitude'].median())\n",
        "                lat_filled = X_transformed['latitude'].fillna(lat_fill_value)\n",
        "                lon_filled = X_transformed['longitude'].fillna(lon_fill_value)\n",
        "\n",
        "                # Simple geographic binning\n",
        "                X_transformed['lat_bin'] = pd.cut(lat_filled, bins=5, labels=False)\n",
        "                X_transformed['lon_bin'] = pd.cut(lon_filled, bins=5, labels=False)\n",
        "\n",
        "        return X_transformed\n",
        "\n",
        "# Prepare features and target - basic cleaning only (no imputation)\n",
        "def prepare_features_basic(df):\n",
        "    df_prep = df.copy()\n",
        "    # Only handle amenities since it's string-based\n",
        "    df_prep['amenities'] = df_prep['amenities'].fillna('')\n",
        "    return df_prep\n",
        "\n",
        "df_prepared = prepare_features_basic(df)\n",
        "\n",
        "# Select features for modeling(columns that are deemed good enough for indication)\n",
        "feature_columns = ['bedrooms', 'bathrooms', 'square_feet', 'amenities', 'cityname', 'state', 'latitude', 'longitude']\n",
        "X = df_prepared[feature_columns].copy()\n",
        "y = df_prepared['price'].copy()\n",
        "\n",
        "print(f\"Features selected: {feature_columns}\")\n",
        "print(f\"Target variable: price\")\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "\n",
        "# Split data FIRST before any preprocessing that could cause data leakage\n",
        "print(\"\\nSplitting data before preprocessing to prevent data leakage...\")\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train_raw.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test_raw.shape[0]} samples\")\n",
        "\n",
        "# Now do preprocessing based ONLY on training data\n",
        "def prepare_features(df_train, df_test=None):\n",
        "    \"\"\"Prepare features using training data statistics only\"\"\"\n",
        "    df_train_prep = df_train.copy()\n",
        "\n",
        "    # Calculate statistics from training data only\n",
        "    train_stats = {\n",
        "        'bedrooms_median': df_train_prep['bedrooms'].median(),\n",
        "        'bathrooms_median': df_train_prep['bathrooms'].median(),\n",
        "        'square_feet_median': df_train_prep['square_feet'].median(),\n",
        "        'latitude_median': df_train_prep['latitude'].median(),\n",
        "        'longitude_median': df_train_prep['longitude'].median()\n",
        "    }\n",
        "\n",
        "    # Fill training data\n",
        "    df_train_prep['bedrooms'] = df_train_prep['bedrooms'].fillna(train_stats['bedrooms_median'])\n",
        "    df_train_prep['bathrooms'] = df_train_prep['bathrooms'].fillna(train_stats['bathrooms_median'])\n",
        "    df_train_prep['square_feet'] = df_train_prep['square_feet'].fillna(train_stats['square_feet_median'])\n",
        "    df_train_prep['amenities'] = df_train_prep['amenities'].fillna('')\n",
        "\n",
        "    if df_test is not None:\n",
        "        df_test_prep = df_test.copy()\n",
        "        # Fill test data using training statistics\n",
        "        df_test_prep['bedrooms'] = df_test_prep['bedrooms'].fillna(train_stats['bedrooms_median'])\n",
        "        df_test_prep['bathrooms'] = df_test_prep['bathrooms'].fillna(train_stats['bathrooms_median'])\n",
        "        df_test_prep['square_feet'] = df_test_prep['square_feet'].fillna(train_stats['square_feet_median'])\n",
        "        df_test_prep['amenities'] = df_test_prep['amenities'].fillna('')\n",
        "        return df_train_prep, df_test_prep, train_stats\n",
        "\n",
        "    return df_train_prep, train_stats\n",
        "\n",
        "X_train_prep, X_test_prep, train_stats = prepare_features(X_train_raw, X_test_raw)\n",
        "\n",
        "# Apply feature engineering (fit on training, transform both)\n",
        "feature_engineer = ApartmentFeatureEngineer(use_new_features=True)\n",
        "feature_engineer.fitted_values.update(train_stats)\n",
        "feature_engineer.fit(X_train_prep)\n",
        "\n",
        "X_train_engineered = feature_engineer.transform(X_train_prep)\n",
        "X_test_engineered = feature_engineer.transform(X_test_prep)\n",
        "\n",
        "print(f\"After feature engineering - Train: {X_train_engineered.shape}, Test: {X_test_engineered.shape}\")\n",
        "print(\"New features created:\", [col for col in X_train_engineered.columns if col not in feature_columns])\n",
        "\n",
        "# Manual preprocessing\n",
        "def preprocess_data(X_train_eng, X_test_eng, use_engineered_features=True):\n",
        "    X_train_processed = X_train_eng.copy()\n",
        "    X_test_processed = X_test_eng.copy()\n",
        "\n",
        "    # Fit label encoders on training data only\n",
        "    le_city = LabelEncoder()\n",
        "    le_state = LabelEncoder()\n",
        "\n",
        "    X_train_processed['cityname_encoded'] = le_city.fit_transform(X_train_processed['cityname'].astype(str))\n",
        "    X_train_processed['state_encoded'] = le_state.fit_transform(X_train_processed['state'].astype(str))\n",
        "\n",
        "    # Transform test data (handle unseen labels)\n",
        "    def safe_transform(encoder, values):\n",
        "        result = []\n",
        "        for val in values.astype(str):\n",
        "            try:\n",
        "                result.append(encoder.transform([val])[0])\n",
        "            except ValueError:\n",
        "                # Assign most frequent class for unseen labels\n",
        "                result.append(0)\n",
        "        return np.array(result)\n",
        "\n",
        "    X_test_processed['cityname_encoded'] = safe_transform(le_city, X_test_processed['cityname'])\n",
        "    X_test_processed['state_encoded'] = safe_transform(le_state, X_test_processed['state'])\n",
        "\n",
        "    # Select final features for modeling based on what's available\n",
        "    basic_features = ['bedrooms', 'bathrooms', 'square_feet', 'latitude', 'longitude',\n",
        "                     'cityname_encoded', 'state_encoded']\n",
        "\n",
        "    if use_engineered_features:\n",
        "        # Only include engineered features if they exist in the dataframe\n",
        "        engineered_features = []\n",
        "        potential_features = ['amenity_count', 'has_square_feet', 'high_cost_location', 'medium_cost_location',\n",
        "                             'total_rooms', 'sqft_log', 'has_dishwasher', 'has_elevator', 'has_pool',\n",
        "                             'has_parking', 'room_efficiency', 'is_studio', 'lat_bin', 'lon_bin']\n",
        "        for feat in potential_features:\n",
        "            if feat in X_train_processed.columns:\n",
        "                engineered_features.append(feat)\n",
        "\n",
        "        final_features = basic_features + engineered_features\n",
        "    else:\n",
        "        final_features = basic_features\n",
        "\n",
        "    X_train_final = X_train_processed[final_features].copy()\n",
        "    X_test_final = X_test_processed[final_features].copy()\n",
        "\n",
        "    # Fill any remaining missing values using training data statistics\n",
        "    for col in X_train_final.columns:\n",
        "        if X_train_final[col].dtype in ['float64', 'int64']:\n",
        "            train_median = X_train_final[col].median()\n",
        "            X_train_final[col] = X_train_final[col].fillna(train_median)\n",
        "            X_test_final[col] = X_test_final[col].fillna(train_median)\n",
        "        else:\n",
        "            train_mode = X_train_final[col].mode()\n",
        "            fill_value = train_mode.iloc[0] if len(train_mode) > 0 else 0\n",
        "            X_train_final[col] = X_train_final[col].fillna(fill_value)\n",
        "            X_test_final[col] = X_test_final[col].fillna(fill_value)\n",
        "\n",
        "    return X_train_final, X_test_final, le_city, le_state\n",
        "\n",
        "X_train_processed, X_test_processed, label_encoder_city, label_encoder_state = preprocess_data(\n",
        "    X_train_engineered, X_test_engineered, use_engineered_features=True\n",
        ")\n",
        "\n",
        "print(f\"Final processed features shape - Train: {X_train_processed.shape}, Test: {X_test_processed.shape}\")\n",
        "print(\"Final features:\", list(X_train_processed.columns))\n",
        "\n",
        "# Create price categories for Naive Bayes classification using ONLY training data\n",
        "print(\"\\nCreating price categories for Naive Bayes classification using training data only...\")\n",
        "\n",
        "# Strategy 1: Try fewer categories first (3 instead of 5) - use training data quantiles\n",
        "print(\"Testing different binning strategies using training data quantiles...\")\n",
        "\n",
        "# 3-category binning (more samples per category) - based on training data only\n",
        "price_quantiles_3 = y_train.quantile([0.33, 0.67]).values\n",
        "price_bins_3 = [-np.inf] + list(price_quantiles_3) + [np.inf]\n",
        "price_labels_3 = ['Low', 'Medium', 'High']\n",
        "y_train_categorical_3 = pd.cut(y_train, bins=price_bins_3, labels=price_labels_3)\n",
        "y_test_categorical_3 = pd.cut(y_test, bins=price_bins_3, labels=price_labels_3)\n",
        "\n",
        "print(\"3-category distribution (training data):\")\n",
        "print(y_train_categorical_3.value_counts().sort_index())\n",
        "\n",
        "# 5-category binning (original) - based on training data only\n",
        "price_quantiles_5 = y_train.quantile([0.2, 0.4, 0.6, 0.8]).values\n",
        "price_bins_5 = [-np.inf] + list(price_quantiles_5) + [np.inf]\n",
        "price_labels_5 = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
        "y_train_categorical_5 = pd.cut(y_train, bins=price_bins_5, labels=price_labels_5)\n",
        "y_test_categorical_5 = pd.cut(y_test, bins=price_bins_5, labels=price_labels_5)\n",
        "\n",
        "print(\"\\n5-category distribution (training data):\")\n",
        "print(y_train_categorical_5.value_counts().sort_index())\n",
        "\n",
        "# Use 3-category for better performance initially\n",
        "y_train_cat = y_train_categorical_3\n",
        "y_test_cat = y_test_categorical_3\n",
        "price_labels = price_labels_3\n",
        "\n",
        "# Scale the features for better Naive Bayes performance (fit on training, transform both)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
        "X_test_scaled = scaler.transform(X_test_processed)\n",
        "\n",
        "print(\"\\n3. NAIVE BAYES MODEL TRAINING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Initialize Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "print(\"Training Naive Bayes classifier...\")\n",
        "nb_model.fit(X_train_scaled, y_train_cat)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_cat = nb_model.predict(X_test_scaled)\n",
        "y_pred_proba = nb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "print(\"Naive Bayes classification completed!\")\n",
        "\n",
        "# Classification metrics\n",
        "accuracy = accuracy_score(y_test_cat, y_pred_cat)\n",
        "print(f\"\\nClassification Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_cat, y_pred_cat))\n",
        "\n",
        "# Convert categorical predictions back to price estimates for regression metrics\n",
        "print(\"\\nConverting categorical predictions to price estimates...\")\n",
        "\n",
        "# Calculate mean price for each category from training data ONLY\n",
        "category_price_map = {}\n",
        "for category in price_labels:\n",
        "    mask = y_train_cat == category\n",
        "    if mask.sum() > 0:\n",
        "        category_price_map[category] = y_train[mask].mean()\n",
        "    else:\n",
        "        category_price_map[category] = y_train.mean()\n",
        "\n",
        "print(\"Category to price mapping:\")\n",
        "for cat, price in category_price_map.items():\n",
        "    print(f\"  {cat}: ${price:,.2f}\")\n",
        "\n",
        "# Convert predictions to price estimates\n",
        "y_pred_price = [category_price_map[pred] for pred in y_pred_cat]\n",
        "\n",
        "# Calculate regression metrics\n",
        "mse = mean_squared_error(y_test, y_pred_price)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred_price)\n",
        "r2 = r2_score(y_test, y_pred_price)\n",
        "\n",
        "print(f\"\\nRegression Metrics:\")\n",
        "print(f\"RMSE: ${rmse:,.2f}\")\n",
        "print(f\"MAE: ${mae:,.2f}\")\n",
        "print(f\"RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "print(\"\\n4. HYPERPARAMETER TUNING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Parameter grid for Naive Bayes\n",
        "param_grid = {\n",
        "    'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
        "}\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    GaussianNB(),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',  # Using accuracy for classification\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train_cat)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Test different binning strategies\n",
        "print(\"\\nTesting different binning strategies...\")\n",
        "\n",
        "binning_results = {}\n",
        "\n",
        "# Test 3-category binning\n",
        "nb_3 = GaussianNB(var_smoothing=grid_search.best_params_['var_smoothing'])\n",
        "nb_3.fit(X_train_scaled, y_train_categorical_3)\n",
        "acc_3 = accuracy_score(y_test_categorical_3, nb_3.predict(X_test_scaled))\n",
        "binning_results['3-category'] = acc_3\n",
        "\n",
        "# Test 4-category binning - use training data quantiles\n",
        "price_quantiles_4 = y_train.quantile([0.25, 0.5, 0.75]).values\n",
        "price_bins_4 = [-np.inf] + list(price_quantiles_4) + [np.inf]\n",
        "price_labels_4 = ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
        "y_train_categorical_4 = pd.cut(y_train, bins=price_bins_4, labels=price_labels_4)\n",
        "y_test_categorical_4 = pd.cut(y_test, bins=price_bins_4, labels=price_labels_4)\n",
        "\n",
        "nb_4 = GaussianNB(var_smoothing=grid_search.best_params_['var_smoothing'])\n",
        "nb_4.fit(X_train_scaled, y_train_categorical_4)\n",
        "acc_4 = accuracy_score(y_test_categorical_4, nb_4.predict(X_test_scaled))\n",
        "binning_results['4-category'] = acc_4\n",
        "\n",
        "# Test 5-category binning (original)\n",
        "nb_5 = GaussianNB(var_smoothing=grid_search.best_params_['var_smoothing'])\n",
        "nb_5.fit(X_train_scaled, y_train_categorical_5)\n",
        "acc_5 = accuracy_score(y_test_categorical_5, nb_5.predict(X_test_scaled))\n",
        "binning_results['5-category'] = acc_5\n",
        "\n",
        "print(\"Binning strategy results:\")\n",
        "for strategy, accuracy in binning_results.items():\n",
        "    print(f\"  {strategy}: {accuracy:.4f}\")\n",
        "\n",
        "# Use the best binning strategy\n",
        "best_binning = max(binning_results.items(), key=lambda x: x[1])\n",
        "print(f\"\\nBest binning strategy: {best_binning[0]} with accuracy {best_binning[1]:.4f}\")\n",
        "\n",
        "# Update the categorical variables to use the best binning\n",
        "if best_binning[0] == '3-category':\n",
        "    y_train_cat = y_train_categorical_3\n",
        "    y_test_cat = y_test_categorical_3\n",
        "    price_labels = ['Low', 'Medium', 'High']\n",
        "    best_nb_model = nb_3\n",
        "elif best_binning[0] == '4-category':\n",
        "    y_train_cat = y_train_categorical_4\n",
        "    y_test_cat = y_test_categorical_4\n",
        "    price_labels = ['Low', 'Medium-Low', 'Medium-High', 'High']\n",
        "    best_nb_model = nb_4\n",
        "else:\n",
        "    y_train_cat = y_train_categorical_5\n",
        "    y_test_cat = y_test_categorical_5\n",
        "    price_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
        "    best_nb_model = nb_5\n",
        "\n",
        "# Test with and without feature engineering using the best binning\n",
        "print(f\"\\nTesting feature engineering impact with {best_binning[0]} binning...\")\n",
        "\n",
        "# Without feature engineering - reprocess data\n",
        "feature_engineer_basic = ApartmentFeatureEngineer(use_new_features=False)\n",
        "X_train_basic = feature_engineer_basic.transform(X_train_prep)\n",
        "X_test_basic = feature_engineer_basic.transform(X_test_prep)\n",
        "\n",
        "X_train_basic_processed, X_test_basic_processed, _, _ = preprocess_data(\n",
        "    X_train_basic, X_test_basic, use_engineered_features=False\n",
        ")\n",
        "\n",
        "X_train_basic_scaled = scaler.fit_transform(X_train_basic_processed)\n",
        "X_test_basic_scaled = scaler.transform(X_test_basic_processed)\n",
        "\n",
        "# Train basic model with best binning\n",
        "nb_basic = GaussianNB(var_smoothing=grid_search.best_params_['var_smoothing'])\n",
        "nb_basic.fit(X_train_basic_scaled, y_train_cat)\n",
        "y_pred_basic = nb_basic.predict(X_test_basic_scaled)\n",
        "\n",
        "accuracy_basic = accuracy_score(y_test_cat, y_pred_basic)\n",
        "\n",
        "print(f\"Accuracy without feature engineering: {accuracy_basic:.4f}\")\n",
        "print(f\"Accuracy with feature engineering: {best_binning[1]:.4f}\")\n",
        "print(f\"Feature engineering improvement: {best_binning[1] - accuracy_basic:.4f}\")\n",
        "\n",
        "# Use the best model for final evaluation\n",
        "final_pred_cat = best_nb_model.predict(X_test_scaled)\n",
        "final_pred_proba = best_nb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Recalculate category price mapping for the best binning strategy\n",
        "category_price_map = {}\n",
        "for category in price_labels:\n",
        "    mask = y_train_cat == category\n",
        "    if mask.sum() > 0:\n",
        "        category_price_map[category] = y_train[mask].mean()\n",
        "    else:\n",
        "        category_price_map[category] = y_train.mean()\n",
        "\n",
        "final_pred_price = [category_price_map[pred] for pred in final_pred_cat]\n",
        "\n",
        "# Final metrics\n",
        "final_accuracy = accuracy_score(y_test_cat, final_pred_cat)\n",
        "final_mse = mean_squared_error(y_test, final_pred_price)\n",
        "final_rmse = np.sqrt(final_mse)\n",
        "final_mae = mean_absolute_error(y_test, final_pred_price)\n",
        "final_r2 = r2_score(y_test, final_pred_price)\n",
        "\n",
        "print(f\"\\nFinal Tuned Model Performance:\")\n",
        "print(f\"Classification Accuracy: {final_accuracy:.4f}\")\n",
        "print(f\"RMSE: ${final_rmse:,.2f}\")\n",
        "print(f\"MAE: ${final_mae:,.2f}\")\n",
        "print(f\"RÂ² Score: {final_r2:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4rCcH33ry1T",
        "outputId": "4bc2805f-e635-4808-c404-fa5377ab179b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "(10000, 22)\n",
            "\n",
            "2. DATA PREPARATION\n",
            "----------------------------------------\n",
            "Removing 'price_display' \n",
            "Features selected: ['bedrooms', 'bathrooms', 'square_feet', 'amenities', 'cityname', 'state', 'latitude', 'longitude']\n",
            "Target variable: price\n",
            "Feature matrix shape: (10000, 8)\n",
            "Target vector shape: (10000,)\n",
            "\n",
            "Splitting data before preprocessing to prevent data leakage...\n",
            "Training set size: 8000 samples\n",
            "Test set size: 2000 samples\n",
            "After feature engineering - Train: (8000, 22), Test: (2000, 22)\n",
            "New features created: ['has_square_feet', 'sqft_log', 'amenity_count', 'has_dishwasher', 'has_elevator', 'has_pool', 'has_parking', 'high_cost_location', 'medium_cost_location', 'total_rooms', 'room_efficiency', 'is_studio', 'lat_bin', 'lon_bin']\n",
            "Final processed features shape - Train: (8000, 21), Test: (2000, 21)\n",
            "Final features: ['bedrooms', 'bathrooms', 'square_feet', 'latitude', 'longitude', 'cityname_encoded', 'state_encoded', 'amenity_count', 'has_square_feet', 'high_cost_location', 'medium_cost_location', 'total_rooms', 'sqft_log', 'has_dishwasher', 'has_elevator', 'has_pool', 'has_parking', 'room_efficiency', 'is_studio', 'lat_bin', 'lon_bin']\n",
            "\n",
            "Creating price categories for Naive Bayes classification using training data only...\n",
            "Testing different binning strategies using training data quantiles...\n",
            "3-category distribution (training data):\n",
            "price\n",
            "Low       2641\n",
            "Medium    2722\n",
            "High      2637\n",
            "Name: count, dtype: int64\n",
            "\n",
            "5-category distribution (training data):\n",
            "price\n",
            "Very Low     1602\n",
            "Low          1601\n",
            "Medium       1600\n",
            "High         1598\n",
            "Very High    1599\n",
            "Name: count, dtype: int64\n",
            "\n",
            "3. NAIVE BAYES MODEL TRAINING\n",
            "----------------------------------------\n",
            "Training Naive Bayes classifier...\n",
            "Naive Bayes classification completed!\n",
            "\n",
            "Classification Accuracy: 0.5665\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        High       0.66      0.59      0.62       627\n",
            "         Low       0.58      0.77      0.66       696\n",
            "      Medium       0.44      0.34      0.38       677\n",
            "\n",
            "    accuracy                           0.57      2000\n",
            "   macro avg       0.56      0.57      0.56      2000\n",
            "weighted avg       0.56      0.57      0.55      2000\n",
            "\n",
            "\n",
            "Converting categorical predictions to price estimates...\n",
            "Category to price mapping:\n",
            "  Low: $823.82\n",
            "  Medium: $1,281.21\n",
            "  High: $2,379.16\n",
            "\n",
            "Regression Metrics:\n",
            "RMSE: $843.22\n",
            "MAE: $503.61\n",
            "RÂ² Score: 0.2007\n",
            "\n",
            "4. HYPERPARAMETER TUNING\n",
            "----------------------------------------\n",
            "Best parameters: {'var_smoothing': 0.001}\n",
            "Best cross-validation accuracy: 0.5781\n",
            "\n",
            "Testing different binning strategies...\n",
            "Binning strategy results:\n",
            "  3-category: 0.5670\n",
            "  4-category: 0.4480\n",
            "  5-category: 0.3750\n",
            "\n",
            "Best binning strategy: 3-category with accuracy 0.5670\n",
            "\n",
            "Testing feature engineering impact with 3-category binning...\n",
            "Accuracy without feature engineering: 0.5010\n",
            "Accuracy with feature engineering: 0.5670\n",
            "Feature engineering improvement: 0.0660\n",
            "\n",
            "Final Tuned Model Performance:\n",
            "Classification Accuracy: 0.5670\n",
            "RMSE: $843.19\n",
            "MAE: $503.50\n",
            "RÂ² Score: 0.2008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBMzMOlwagAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V40bguIgaMM_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}